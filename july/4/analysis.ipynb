{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../1\")\n",
    "\n",
    "import compare_benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedBenchmarkData = []\n",
    "for i in range(1, 6):\n",
    "    individualLoadedBenchmarkSet = {\n",
    "        \"base_perftests\": compare_benchmarks.load_benchmarks(\"./parallel-variability-results/base_perftests.{i}.json\".format(i=i)),\n",
    "        \"components_perftests\": compare_benchmarks.load_benchmarks(\"./parallel-variability-results/components_perftests.{i}.json\".format(i=i)),\n",
    "        \"browser_tests\": compare_benchmarks.load_benchmarks(\"./parallel-variability-results/browser_tests.{i}.json\".format(i=i))\n",
    "    }\n",
    "    loadedBenchmarkData.append(individualLoadedBenchmarkSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedBenchmarkData = []\n",
    "for loadedBenchmarkSet in loadedBenchmarkData:\n",
    "    benchmarkDataArray = []\n",
    "    for loadedBenchmarkSuite in loadedBenchmarkSet:\n",
    "        benchmarkDataArray.append(loadedBenchmarkSet[loadedBenchmarkSuite])\n",
    "    combinedBenchmarkData.append(compare_benchmarks.combine_benchmarks(benchmarkDataArray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarkSetComparisons = []\n",
    "for benchmarkList in combinedBenchmarkData[1:]:\n",
    "    benchmarkSetComparisons.append(compare_benchmarks.per_test_comparison(combinedBenchmarkData[0], benchmarkList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'mem_uops_retired.all_loads': 1.0000241833956993, 'mem_uops_retired.all_stores': 1.000016037458837}, {'mem_uops_retired.all_loads': 1.0001813825349686, 'mem_uops_retired.all_stores': 1.0000450655330066}, {'mem_uops_retired.all_loads': 1.000142751975567, 'mem_uops_retired.all_stores': 0.9999504170454483}, {'mem_uops_retired.all_loads': 1.0002705459621162, 'mem_uops_retired.all_stores': 1.0000512068267495}]\n"
     ]
    }
   ],
   "source": [
    "overallComparisons = []\n",
    "for benchmarkComparisonSet in benchmarkSetComparisons:\n",
    "    overallComparisons.append(compare_benchmarks.get_average_speedup(benchmarkComparisonSet))\n",
    "print(overallComparisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Based on these results, running everything in parallel (at least using n-2 threads where n is the total number of hardware threads in the system) seems to work decently well and gets to the desired levels of accuracey. Going to continue doing everything like this and should result in somewhat fast benchmarking times while still preserving the run-to-run variability that is needed so that we can actually test the differences that we are expecting to get"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
