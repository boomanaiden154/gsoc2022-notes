{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../1\")\n",
    "\n",
    "import compare_benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarkSets = [\"baseline\", \"candidate-virtreg\", \"existing-model\"]\n",
    "benchmarkSuites = [\"base_perftests\", \"browser_tests\", \"components_perftests\"]\n",
    "allBenchmarks = {}\n",
    "for benchmarkSet in benchmarkSets:\n",
    "    individualBenchmarkSet = []\n",
    "    for benchmarkSuite in benchmarkSuites:\n",
    "        individualBenchmarkSet.append(compare_benchmarks.load_benchmarks(\"./benchmark-results/{set}/{suite}.json\".format(set=benchmarkSet,suite=benchmarkSuite)))\n",
    "    allBenchmarks[benchmarkSet] = compare_benchmarks.combine_benchmarks(individualBenchmarkSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "existingOverBaselineAllTests = compare_benchmarks.per_test_comparison(allBenchmarks[\"baseline\"], allBenchmarks[\"existing-model\"])\n",
    "candidateVirtregOverBaselineAllTests = compare_benchmarks.per_test_comparison(allBenchmarks[\"baseline\"], allBenchmarks[\"candidate-virtreg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mem_uops_retired.all_loads': 0.9912269413386584, 'mem_uops_retired.all_stores': 0.9918548298559658}\n",
      "{'mem_uops_retired.all_loads': 1.0114889761945915, 'mem_uops_retired.all_stores': 1.002015137968915}\n"
     ]
    }
   ],
   "source": [
    "existingOverBaseline = compare_benchmarks.get_average_speedup(existingOverBaselineAllTests)\n",
    "candidateVirtregOverBaseline = compare_benchmarks.get_average_speedup(candidateVirtregOverBaselineAllTests)\n",
    "print(existingOverBaseline)\n",
    "print(candidateVirtregOverBaseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "The model with the new features is still not even beating the baseline, much less the new model. There are a couple reasons for this. One: I messed something up somewhere when training the model/generating the benchmarking results, but I don't think that is it. The new feature could just be bad and require more hidden nodes/layers in order to really process the data, or it could just be that the training strategy that I'm using still isn't working optimally and I either need a bigger corpus (multiproject) or just need to train for longer/choose the best model from a pool of models. Still testing everything again on the `llvm-test-suite` can't really hurt anything, but I'm not really expecting to see results that are a whole lot different."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
