# Viewing Archive Dependencies in Chromium

Currently the Chromium linux build uses a lot of thin/static archives during
the build process. This can present somewhat of a challenge for MLGO corpus
extraction since the current process with local thinLTO ends up missing a lot
of files due to not being able to recognize where the `*.thinlto.bc` and
`*.3.import.bc` files are since they get attached to the archives that are used
in the linking process rather than the object files that are enumerated in the
`compile_commands.json` process spit out by `gn`. This document contains some
notes/documentation during the debug process for fixing this issue in the upstream
[google/ml-compiler-opt](https://github.com/google/ml-compiler-opt) repository.

### Viewing all the archives generated by the build process

To view all archives generated, simply look at the `alink` targets from ninja.
The following command should show everything:

```bash
ninja -t targets all | grep alink
```

### Seeing the contents of an archive

Seeing the contents of an archive is pretty easy using the `ar` (or `llvm-ar` if
using the LLVM binary utilities) and can be done like this:

```bash
llvm-ar -t /path/to/archive.a
```

However, if the archive isn't thin, this will only give us the name of the object
files rather than the path to the original object file. We need the path to the
original object file to make sure we're picking up the exact right object file
and not running into any naming collisions. This can be done if we specify a ninja
build flag and look at the response files that ninja generates (assuming all the
archive targets are setup to use response files). Running ninja like this:

```bash
ninja -d keeprsp <target>
```

Will make ninja keep the `*.rsp` files that are generated during the build process
and used to pass the list of files to the `llvm-ar` tool. This gets us the exact paths
of the object files that we want.

It seems like all of the archives that get created during the Chromium build process
are done using this technique. This might not generalize to other pieces of software,
but it shouldn't be too bad to implement command parsing logic, grabbing all the
commands from ninja. Probably not going to implement it for now though just to get
the Chromium stuff working.

### Full Solution

* Writing a script that interfaces with ninja to grab all the `alink` targets in
the generated ninja files.
* Have this script also take in a `compile_commands.json` and then automatically
match object files from the compile command database with those in archives and
then automatically have it look for `*.thinlto.bc` and `*.3.import.bc` files.
* Make the script export a manifest of files, probably in JSON format, that
can then be used by other scripts.
* Make the `extract_ir.py` script accept this manifest and use it for corpus
extraction.

# Final Conclusion

This turned out to be completely uselss as the current script is just using a
glob within the build folder to find `*.3.import.bc` files and then using that
to extract a corpus. I was passing in some incorrect flags to the script which
caused it to not work correctly. Currently upstreaming a
[patch](https://github.com/google/ml-compiler-opt/pull/179) to make it so that
no one should run into this issue in the future. Leaving the documentation here
mostly for myself in case I need to reference some ninja commands again.